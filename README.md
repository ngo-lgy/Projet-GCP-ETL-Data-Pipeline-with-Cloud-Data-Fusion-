# Projet GCP (ETL Data Pipeline with Cloud Data Fusion)

Dans ce projet, nous verrons comment importer des donnÃ©es dans un bucket Google Cloud Storage (GCS), puis les transformer Ã  lâ€™aide de lâ€™outil visuel Wrangler dans une instance Cloud Data Fusion (Ã©tape de transformation). Ensuite, nous crÃ©erons un pipeline pour charger les donnÃ©es transformÃ©es dans BigQuery (Ã©tape de chargement). Enfin, nous verrons comment connecter Looker Studio (anciennement Google Data Studio) Ã  BigQuery afin de rÃ©aliser des visualisations Ã  partir des donnÃ©es nettoyÃ©es et structurÃ©es.

<img width="601" alt="Airflow01" src="https://github.com/user-attachments/assets/9cc5e917-a9c1-4bfe-a569-b984bf1f0601" />

Nous commenÃ§ons par accÃ©der Ã  la console Google Cloud Platform (GCP). Pour ce projet, j'utilise l'essai gratuit, comme cela a Ã©tÃ© le cas pour le projet **MSBI SSIS (ETL)** que je vous invite Ã  consulter dans la section *Projets* de mon profil.

### CrÃ©ation de lâ€™instance Cloud Data Fusion

Je commence par crÃ©er une instance **Cloud Data Fusion**, qui nous permettra dâ€™orchestrer notre pipeline ETL Ã  lâ€™aide de son interface visuelle.

`--------data_fusion0--------`

Pendant les 20 minutes nÃ©cessaires Ã  la crÃ©ation de lâ€™instance, je crÃ©e un **bucket GCS (Google Cloud Storage)**, qui servira de source de donnÃ©es. Pour cela, je recherche â€œCloud Storageâ€ dans la barre de navigation GCP.

`--------bucket1--------`

Une fois le bucket crÃ©Ã©, jâ€™y importe mon fichier CSV nommÃ© **employee\_data.csv**, qui contient les donnÃ©es brutes Ã  traiter.

`--------bucket2--------`

---

### Lancement de lâ€™environnement Wrangler

Lorsque lâ€™instance Cloud Data Fusion est prÃªte, je clique sur **"Afficher lâ€™instance"**. AprÃ¨s avoir acceptÃ© les conditions dâ€™utilisation liÃ©es au compte, jâ€™accÃ¨de Ã  **Wrangler**, lâ€™outil visuel de transformation de donnÃ©es.

`--------wrangler3--------`

Dans Wrangler, je retrouve mon bucket et le fichier importÃ©.

* Vue du dossier contenant les donnÃ©es :
  `--------wrangler4--------`

* Vue des donnÃ©es du fichier CSV :
  `--------wrangler5--------`

---

### ğŸ› ï¸ PrÃ©paration et transformation des donnÃ©es (Wrangler)

Dans cet espace visuel, jâ€™effectue les Ã©tapes de **prÃ©paration des donnÃ©es** :

* ğŸ”¤ **CrÃ©ation d'une colonne Full\_name** : en concatÃ©nant les colonnes `first_name` et `last_name`
* ğŸ”’ **Masquage des donnÃ©es sensibles** : par exemple, en **masquant les salaires** avec une transformation de type â€œmaskâ€
* ğŸ” **Encodage des mots de passe** : par **hachage MD5** pour les sÃ©curiser

AperÃ§u des transformations appliquÃ©es dans Wrangler :

`--------wrangler6--------`

---

### âš™ï¸ CrÃ©ation et dÃ©ploiement du pipeline ETL

Une fois les transformations terminÃ©es, je clique en haut Ã  droite sur **â€œCreate a pipelineâ€**, puis je choisis **â€œBatch pipelineâ€**. Cela mâ€™ouvre lâ€™interface de crÃ©ation du pipeline.

`--------pipeline7--------`

Dans cette interface :

1. Je crÃ©e une destination (sink) **BigQuery**
2. Jâ€™indique le **nom du dataset** et **lâ€™identifiant du projet**
3. Je donne un **nom au pipeline**
4. Puis je clique sur **â€œDeployâ€** pour le dÃ©ployer

`--------pipeline8--------`

Une fois dÃ©ployÃ©, je suis redirigÃ© vers lâ€™Ã©cran dâ€™exÃ©cution oÃ¹ je peux lancer le pipeline.

`--------pipeline9--------`

---

### âš ï¸ Avant d'exÃ©cuter le pipeline, penser Ã  :

* **Activer lâ€™API Cloud Resource Manager**
* **Attribuer les bonnes permissions IAM** Ã  lâ€™instance :

  * RÃ´le **Dataproc Editor**
  * RÃ´le **Storage Admin (ou Storage Object Admin)**
* **Passer les buckets en mode â€œautorisations uniformesâ€** dans GCS

Une fois le pipeline exÃ©cutÃ© avec succÃ¨s (*Status: Succeeded*), je peux revenir dans BigQuery, actualiser la page et vÃ©rifier que mon dataset et ma table ont bien Ã©tÃ© crÃ©Ã©s.

`--------bigquery_table10--------`

---

### ğŸ“Š Visualisation des donnÃ©es avec Looker Studio

Enfin, je connecte **BigQuery Ã  Looker Studio** (anciennement Google Data Studio) pour crÃ©er des tableaux de bord Ã  partir des donnÃ©es transformÃ©es.

`--------looker_studio11--------`

Exemple de visualisation gÃ©nÃ©rÃ©e Ã  partir du champ `full_name` :

`--------looker_studio12--------`
