# Projet GCP (ETL Data Pipeline with Cloud Data Fusion)

Dans ce projet, nous mettons en place un pipeline de donnÃ©es complet, de l'importation de fichiers dans Google Cloud Storage (GCS) Ã  leur transformation via Cloud Data Fusion (Wrangler), jusqu'au chargement dans BigQuery. Enfin, nous connectons Looker Studio (anciennement Google Data Studio) Ã  BigQuery pour crÃ©er des visualisations sur les donnÃ©es chargÃ©es.

<img width="600" alt="pipeline00" src="https://github.com/user-attachments/assets/e19aab41-fe5a-4f28-abdd-f262d1960e47" />


Nous commenÃ§ons par accÃ©der Ã  la console Google Cloud Platform (GCP). Pour ce projet, j'utilise l'essai gratuit, comme cela a Ã©tÃ© le cas pour le projet **MSBI SSIS (ETL)** que je vous invite Ã  consulter dans la section *Projets* de mon profil.

### CrÃ©ation de lâ€™instance Cloud Data Fusion

Je commence par crÃ©er une instance **Cloud Data Fusion**, qui nous permettra dâ€™orchestrer notre pipeline ETL Ã  lâ€™aide de son interface visuelle.

<img width="959" alt="data_fusion0" src="https://github.com/user-attachments/assets/73dea6b8-aabf-4ffc-b898-76dbfd2b5b14" />

Pendant les 20 minutes nÃ©cessaires Ã  la crÃ©ation de lâ€™instance, je crÃ©e un **bucket GCS (Google Cloud Storage)**, qui servira de source de donnÃ©es. Pour cela, je recherche â€œCloud Storageâ€ dans la barre de navigation GCP.

<img width="958" alt="bucket1" src="https://github.com/user-attachments/assets/62f6ecfa-c3f7-40e6-9792-d20fca004809" />

Une fois le bucket crÃ©Ã©, jâ€™y importe mon fichier CSV nommÃ© **employee\_data.csv**, qui contient les donnÃ©es brutes Ã  traiter.

<img width="958" alt="bucket2" src="https://github.com/user-attachments/assets/19efdf51-03ac-4c55-93c3-39a227bffb76" />

---

### Lancement de lâ€™environnement Wrangler

Lorsque lâ€™instance Cloud Data Fusion est prÃªte, je clique sur **"Afficher lâ€™instance"**. AprÃ¨s avoir acceptÃ© les conditions dâ€™utilisation liÃ©es au compte, jâ€™accÃ¨de Ã  **Wrangler**, lâ€™outil visuel de transformation de donnÃ©es.

<img width="956" alt="warngler3" src="https://github.com/user-attachments/assets/651d3ab3-e0f5-4f52-a33b-5fa04a4f8817" />

Dans Wrangler, je retrouve mon bucket et le fichier importÃ©.

* Vue du dossier contenant les donnÃ©es :
 
<img width="959" alt="wrangler4" src="https://github.com/user-attachments/assets/fc78b93b-ea3c-4c06-9d05-d54499f1cdd0" />

* Vue des donnÃ©es du fichier CSV :
  
<img width="959" alt="wrangler5" src="https://github.com/user-attachments/assets/960983bb-b617-413c-9bcf-5ee523daad58" />

---

### ğŸ› ï¸ PrÃ©paration et transformation des donnÃ©es (Wrangler)

Dans cet espace visuel, jâ€™effectue les Ã©tapes de **prÃ©paration des donnÃ©es** :

* ğŸ”¤ **CrÃ©ation d'une colonne Full\_name** : en concatÃ©nant les colonnes first_name et`last_name
* ğŸ”’ **Masquage des donnÃ©es sensibles** : par exemple, en **masquant les salaires** avec une transformation de type â€œmaskâ€
* ğŸ” **Encodage des mots de passe** : par **hachage MD5** pour les sÃ©curiser

AperÃ§u des transformations appliquÃ©es dans Wrangler :


<img width="718" alt="wrangler6" src="https://github.com/user-attachments/assets/a1b0c25f-1c92-4080-838b-b9d9e496f097" />

---

### âš™ï¸ CrÃ©ation et dÃ©ploiement du pipeline ETL

Une fois les transformations terminÃ©es, je clique en haut Ã  droite sur **â€œCreate a pipelineâ€**, puis je choisis **â€œBatch pipelineâ€**. Cela mâ€™ouvre lâ€™interface de crÃ©ation du pipeline.

<img width="958" alt="pipeline7" src="https://github.com/user-attachments/assets/76d1ac28-f439-4195-a3de-4f967ccb9f11" />

Dans cette interface :

1. Je crÃ©e une destination (sink) **BigQuery**
2. Jâ€™indique le **nom du dataset** et **lâ€™identifiant du projet**
3. Je donne un **nom au pipeline**
4. Puis je clique sur **â€œDeployâ€** pour le dÃ©ployer

<img width="957" alt="pipeline8" src="https://github.com/user-attachments/assets/b3563120-8fe6-40c1-b9f5-f15442cdb642" />

Une fois dÃ©ployÃ©, je suis redirigÃ© vers lâ€™Ã©cran dâ€™exÃ©cution oÃ¹ je peux lancer le pipeline.

<img width="956" alt="pipeline9" src="https://github.com/user-attachments/assets/cf10d873-7bbf-4a6d-bf71-b9d8f5901d56" />

---

### âš ï¸ Avant d'exÃ©cuter le pipeline, penser Ã  :

* **Activer lâ€™API Cloud Resource Manager**
* **Attribuer les bonnes permissions IAM** Ã  lâ€™instance :

  * RÃ´le **Dataproc Editor**
  * RÃ´le **Storage Admin (ou Storage Object Admin)**
* **Passer les buckets en mode â€œautorisations uniformesâ€** dans GCS

Une fois le pipeline exÃ©cutÃ© avec succÃ¨s (*Status: Succeeded*), je peux revenir dans BigQuery, actualiser la page et vÃ©rifier que mon dataset et ma table ont bien Ã©tÃ© crÃ©Ã©s.

<img width="957" alt="bigquery_table10" src="https://github.com/user-attachments/assets/df088f62-fcaa-4422-ae28-e207453a9cc7" />

---

### ğŸ“Š Visualisation des donnÃ©es avec Looker Studio

Enfin, je connecte **BigQuery Ã  Looker Studio** (anciennement Google Data Studio) pour crÃ©er des tableaux de bord Ã  partir des donnÃ©es transformÃ©es.

<img width="959" alt="looker_studio11" src="https://github.com/user-attachments/assets/e7512fc9-72aa-4d75-aa6d-9ef324460917" />

Exemple de visualisation gÃ©nÃ©rÃ©e Ã  partir du champ full_name :

<img width="455" alt="looker_studio12" src="https://github.com/user-attachments/assets/e80e7969-7722-4c42-b0c0-0ec1eb4e6a82" />

<img width="959" alt="looker_studio13" src="https://github.com/user-attachments/assets/57b6fb6f-d6e3-4bba-a8e5-39cbba199d52" />
